{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# 카메라 연결\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 긍정적, 부정적 표정 프레임 수 초기화\n",
    "positive_frames = 0\n",
    "negative_frames = 0\n",
    "\n",
    "while True:\n",
    "    # 프레임 읽기\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # DeepFace를 사용하여 표정 분석\n",
    "    result = DeepFace.analyze(frame, actions=['emotion'])\n",
    "\n",
    "    # 감정 결과에서 'emotion' 키의 값을 가져옴\n",
    "    emotion = result['emotion']\n",
    "\n",
    "    # 화면에 표정 출력\n",
    "    cv2.putText(frame, emotion, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # 긍정적, 부정적 표정에 따라 프레임 수 증가\n",
    "    if emotion == 'happy' or emotion == 'surprise':\n",
    "        positive_frames += 1\n",
    "    else:\n",
    "        negative_frames += 1\n",
    "\n",
    "    # 화면 표시\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    # 종료 키 확인\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 비디오 캡쳐 해제 및 윈도우 종료\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 총 프레임에서 긍정적, 부정적 표정 비율 계산\n",
    "total_frames = positive_frames + negative_frames\n",
    "positive_ratio = positive_frames / total_frames\n",
    "negative_ratio = negative_frames / total_frames\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"긍정적 표정 비율: {positive_ratio:.2%}\")\n",
    "print(f\"부정적 표정 비율: {negative_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914063d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'emotion': {'angry': 6.4261903071188184e-21, 'disgust': 0.0, 'fear': 1.158544361752357e-26, 'happy': 100.0, 'sad': 5.2277354594135616e-17, 'surprise': 4.701529790485104e-09, 'neutral': 4.417938459511106e-07}, 'dominant_emotion': 'happy', 'region': {'x': 345, 'y': 211, 'w': 769, 'h': 769}}]\n",
      "감정: happy\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# 이미지 경로 지정\n",
    "image_path = \"imgs/img1.jpg\"\n",
    "\n",
    "# 프레임 지정\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "# DeepFace를 통한 표정 인식\n",
    "emotion = DeepFace.analyze(frame, actions=['emotion'])\n",
    "\n",
    "# \n",
    "print(emotion)\n",
    "\n",
    "result_dict = emotion[0]\n",
    "\n",
    "# 감정 결과에서 'emotion' 키의 값을 가져옴\n",
    "emotion = result_dict['dominant_emotion']\n",
    "\n",
    "\n",
    "# 화면에 표정 출력\n",
    "cv2.putText(frame, emotion, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "# 화면 표시\n",
    "cv2.imshow('Emotion Detection', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"감정: {emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4983e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
