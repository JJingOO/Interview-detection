{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실시간 면접자 얼굴 및 표정 인식\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# 카메라 연결\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# 면접자 인증용\n",
    "img1_path = \"imgs/Verify1.jpg\"\n",
    "model_name = \"Facenet\"\n",
    "\n",
    "# 긍정적, 부정적 표정 프레임수 선언 및 0으로 초기화\n",
    "positive_frames = 0\n",
    "negative_frames = 0\n",
    "\n",
    "# 종료키('q')가 눌리기 전까지 계속 반복\n",
    "while True:\n",
    "    # 프레임 읽기\n",
    "    ret, frame = cam.read()\n",
    "    \n",
    "    # DeepFace를 통한 면접자 인식\n",
    "    # img1과 image2(현재 프레임)가 같으면 True\n",
    "    # model_name 파라미터로 인식 모델 지정 가능\n",
    "    # (VGG-Face, FaceNet, OpenFace, DeepFace, DeepID, Dlib, ArcFace, Ensemble)\n",
    "    resultV = DeepFace.verify(img1_path = img1_path, img2_path = frame, model_name = model_name)\n",
    "    \n",
    "    # 얼굴 인식 결과값 출력,'verified' 추출 및 인증 결과 출력\n",
    "    print(resultV)\n",
    "    verified = resultV['verified']\n",
    "    print(f\"인증: {verified}\")\n",
    "\n",
    "    # DeepFace를 통한 표정 인식\n",
    "    resultE = DeepFace.analyze(frame, actions=['emotion'])\n",
    "\n",
    "    # 인식이 완료된 결과값 출력 후 'emotion'을 추출\n",
    "    print(resultE)\n",
    "    emotion = resultE[0]\n",
    "    \n",
    "    # 저장된 값에서 dominant_emotion'(가장 강한 감정)을 추출 및 출력\n",
    "    emotion = emotion['dominant_emotion']\n",
    "    print(f\"감정: {emotion}\")\n",
    "    \n",
    "    # 긍정적, 부정적 표정 반별 후 프레임수 증가\n",
    "    # (웃는 표정, 혹은 침착 담담한 중립적 표정일 시 긍정 프레임 + 1)\n",
    "    if emotion == 'happy' or emotion == 'neutral':\n",
    "        positive_frames += 1\n",
    "    else:\n",
    "        negative_frames += 1\n",
    "        \n",
    "    total_frames = positive_frames + negative_frames\n",
    "\n",
    "    # 얼굴 인식 결과(bool -> 문자열) + 표정 인식 결과 = 최종 결과\n",
    "    result = '인증 : ' + str(verified) + ' / 감정 : ' + emotion + ' / 총 프레임수 : ' + total_frames + '(' + positive_frames + '/' + negative_frames + ')'\n",
    "    \n",
    "    # 화면에 표정 출력\n",
    "    cv2.putText(frame, result, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # 화면 표시\n",
    "    cv2.imshow('Interview Emotion Detection', frame)\n",
    "\n",
    "    # 'q' 입력 시 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 비디오 캡쳐 해제 및 윈도우 종료\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 총 프레임에서 긍정적, 부정적 표정 비율 계산\n",
    "total_frames = positive_frames + negative_frames\n",
    "positive_ratio = positive_frames / total_frames\n",
    "negative_ratio = negative_frames / total_frames\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"긍정적 표정 비율: {positive_ratio:.2%}\")\n",
    "print(f\"부정적 표정 비율: {negative_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c674058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verified': True, 'distance': 0.24256997707762307, 'threshold': 0.4, 'model': 'Facenet', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 345, 'y': 211, 'w': 769, 'h': 769}, 'img2': {'x': 516, 'y': 192, 'w': 512, 'h': 512}}, 'time': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'emotion': {'angry': 6.4261903071188184e-21, 'disgust': 0.0, 'fear': 1.158544361752357e-26, 'happy': 100.0, 'sad': 5.2277354594135616e-17, 'surprise': 4.701529790485104e-09, 'neutral': 4.417938459511106e-07}, 'dominant_emotion': 'happy', 'region': {'x': 345, 'y': 211, 'w': 769, 'h': 769}}]\n",
      "인증: True\n",
      "감정: happy\n"
     ]
    }
   ],
   "source": [
    "# 단일 이미지 얼굴 및 표정 인식\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# 이미지 경로 지정\n",
    "img1_path = \"imgs/img1.jpg\"\n",
    "img2_path = \"imgs/img2.jpg\"\n",
    "\n",
    "model_name = \"Facenet\"\n",
    "\n",
    "# 프레임 지정\n",
    "frame = cv2.imread(img1_path)\n",
    "\n",
    "resultV = DeepFace.verify(img1_path = img1_path, img2_path =img2_path , model_name = model_name)\n",
    "\n",
    "print(resultV)\n",
    "verified = resultV['verified']\n",
    "\n",
    "# DeepFace를 통한 표정 인식\n",
    "resultE = DeepFace.analyze(frame, actions=['emotion'])\n",
    "\n",
    "# 인식이 완료된 결과값 출력 후 'emotion'을 추출\n",
    "print(resultE)\n",
    "emotion = resultE[0]\n",
    "\n",
    "# 저장된 값에서 dominant_emotion'(가장 강한 감정)을 추출\n",
    "emotion = emotion['dominant_emotion']\n",
    "\n",
    "result = str(verified) + ' / ' + emotion\n",
    "\n",
    "# 화면에 표정 출력\n",
    "cv2.putText(frame, result, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "# 화면 표시\n",
    "cv2.imshow('Interview Emotion Detection', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"인증: {verified}\")\n",
    "print(f\"감정: {emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fea8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
